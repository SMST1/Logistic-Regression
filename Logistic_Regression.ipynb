{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " **1.What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "\n",
        "Logistic Regression and Linear Regression are both supervised machine learning algorithms used for prediction, but they are used in different contexts and have key differences:\n",
        "\n",
        "🔹 Logistic Regression\n",
        "Used for: Classification problems (predicting categories like \"spam\" vs. \"not spam\").\n",
        "\n",
        "Output: Probability between 0 and 1, which is then mapped to class labels (e.g., using a threshold like 0.5).\n",
        "\n",
        "Core Function: Applies the sigmoid function to the linear combination of inputs:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Goal: Estimate the probability that a given input belongs to a specific class.\n",
        "\n",
        "🔹 Linear Regression\n",
        "Used for: Regression problems (predicting continuous values like house price).\n",
        "\n",
        "Output: Continuous value (e.g., 82.5).\n",
        "\n",
        "Core Function: Fits a straight line (or hyperplane) to the data:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Goal: Minimize the difference between predicted and actual values (typically using Mean Squared Error).\n",
        "\n",
        "**2.What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "The mathematical equation of Logistic Regression models the probability that a given input belongs to a certain class (usually class 1) using the sigmoid function applied to a linear combination of input features:\n",
        "\n",
        "✅ Logistic Regression Equation\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "P(y=1∣x)=σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "σ(z) is the sigmoid (logistic) function\n",
        "\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept (bias term)\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  are the coefficients (weights)\n",
        "\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        "  are the input features\n",
        "\n",
        "**3. Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "We use the sigmoid function in logistic regression because it transforms any real-valued number into a value between 0 and 1, which makes it perfect for modeling probabilities.\n",
        "\n",
        "🔍 Here's Why the Sigmoid Function is Used:\n",
        "✅ 1. Probability Output\n",
        "Probabilities must lie between 0 and 1.\n",
        "\n",
        "The sigmoid function:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "ensures that no matter what the value of\n",
        "𝑧\n",
        "z (which can be any real number), the output is always between 0 and 1.\n",
        "\n",
        "✅ 2. Smooth & Differentiable\n",
        "It's smooth and has a nice S-shaped curve, which is helpful for gradient-based optimization (like gradient descent).\n",
        "\n",
        "✅ 3. Natural Fit for Classification\n",
        "In binary classification, we want a decision boundary — the sigmoid naturally creates a threshold (commonly at 0.5) to separate classes.\n",
        "\n",
        "✅ 4. Log Odds Interpretation\n",
        "Logistic regression models the log-odds of the outcome as a linear function:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "1\n",
        "−\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−P\n",
        "P\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Applying the inverse of this log-odds function gives us the sigmoid — so it's mathematically consistent.\n",
        "\n",
        "\n",
        "**4. What is the cost function of Logistic Regression?**\n",
        "\n",
        "\n",
        " Logistic Regression Cost Function\n",
        "Also known as Log Loss or Binary Cross-Entropy, it is defined as:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "Where:\n",
        "\n",
        "𝑚\n",
        "m = number of training examples\n",
        "\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  = actual label (0 or 1)\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ) = predicted probability (from the sigmoid function)\n",
        "\n",
        "𝜃\n",
        "θ = vector of model parameters\n",
        "\n",
        "**5. What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the cost function that discourages overly complex models.\n",
        "Regularization modifies the cost function to include a term that penalizes large coefficients:\n",
        "\n",
        "It is needed for :\n",
        "\n",
        "Prevents Overfitting\n",
        "\n",
        "When the model fits the training data too well, it may fail to generalize to new data.\n",
        "\n",
        "Regularization keeps weights smaller, simplifying the model.\n",
        "\n",
        "Encourages Simplicity\n",
        "\n",
        "Smaller weights → less reliance on specific features → more robust predictions.\n",
        "\n",
        "Helps with Multicollinearity\n",
        "\n",
        "If input features are correlated, regularization can stabilize the solution.\n",
        "\n",
        "**6. Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
        "\n",
        "The differences between Lasso, Ridge, and Elastic Net regression — all of which are regularization techniques used to prevent overfitting in models like linear and logistic regression.\n",
        "\n",
        "1. Ridge Regression (L2 Regularization)\n",
        "Penalty Term:\n",
        "\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect:\n",
        "Shrinks coefficients toward zero, but never exactly to zero.\n",
        "\n",
        "Use When:\n",
        "You want to reduce model complexity but keep all features.\n",
        "\n",
        "🔹 2. Lasso Regression (L1 Regularization)\n",
        "Penalty Term:\n",
        "\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Effect:\n",
        "Shrinks some coefficients to exactly zero, effectively selecting features.\n",
        "\n",
        "Use When:\n",
        "You want both regularization and feature selection.\n",
        "\n",
        "🔹 3. Elastic Net Regression\n",
        "Penalty Term:\n",
        "\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "or equivalently:\n",
        "\n",
        "𝜆\n",
        "(\n",
        "𝛼\n",
        "∑\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "∑\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        ")\n",
        "λ(α∑∣θ\n",
        "j\n",
        "​\n",
        " ∣+(1−α)∑θ\n",
        "j\n",
        "2\n",
        "​\n",
        " )\n",
        "Effect:\n",
        "Combines the strengths of both Ridge and Lasso — it can shrink coefficients and perform feature selection.\n",
        "\n",
        "Use When:\n",
        "You have many correlated features or want a balance between Ridge and Lasso.\n",
        "\n",
        "**7. When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "We should use Elastic Net instead of just Lasso or Ridge when we want the best of both worlds — especially in situations where neither Lasso nor Ridge alone performs well.\n",
        "\n",
        "Use Elastic Net When:\n",
        "1. You Have Many Features That Are Correlated\n",
        "Lasso tends to randomly pick one and ignore the rest.\n",
        "\n",
        "Elastic Net can select groups of correlated features together, thanks to the Ridge component.\n",
        "\n",
        "2. You Have More Features Than Observations\n",
        "Example: gene expression data (thousands of features, few samples).\n",
        "\n",
        "Lasso alone may break down, but Elastic Net handles it more gracefully.\n",
        "\n",
        "3. You Want Both Regularization and Feature Selection\n",
        "Ridge shrinks coefficients but keeps all features.\n",
        "\n",
        "Lasso does feature selection but can be unstable with correlated features.\n",
        "\n",
        "Elastic Net shrinks and selects while being more stable.\n",
        "\n",
        "4. Model Performance is Unstable with Lasso or Ridge Alone\n",
        "If Lasso gives too sparse a model or Ridge keeps too many unimportant features, Elastic Net often finds a better trade-off.\n",
        "\n",
        "**8. What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
        "\n",
        "The regularization parameter\n",
        "𝜆\n",
        "λ in logistic regression controls how much penalty is applied to the model’s coefficients. It directly impacts the model’s complexity and performance.\n",
        "\n",
        "🔍 Impact of\n",
        "𝜆\n",
        "λ:\n",
        "✅ 1. Large\n",
        "𝜆\n",
        "λ (High Regularization):\n",
        "Heavily penalizes large weights.\n",
        "\n",
        "Coefficients shrink closer to zero.\n",
        "\n",
        "Reduces overfitting (high bias, low variance).\n",
        "\n",
        "May underfit the data if too strong.\n",
        "\n",
        "✅ 2. Small\n",
        "𝜆\n",
        "λ (Low Regularization):\n",
        "Penalty is weaker, so weights can grow.\n",
        "\n",
        "Model fits the training data more closely.\n",
        "\n",
        "Risk of overfitting (low bias, high variance).\n",
        "\n",
        "✅ 3.\n",
        "𝜆\n",
        "=\n",
        "0\n",
        "λ=0:\n",
        "No regularization at all.\n",
        "\n",
        "Logistic regression behaves like standard maximum likelihood estimation.\n",
        "\n",
        "High risk of overfitting, especially with many features.\n",
        "\n",
        "\n",
        "**9. What are the key assumptions of Logistic Regression?**\n",
        "\n",
        "Key Assumptions of Logistic Regression:\n",
        "1. Linearity of Logit (Not the Outcome)\n",
        "The model assumes a linear relationship between the independent variables and the log-odds of the dependent variable:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "1\n",
        "−\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−P\n",
        "P\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Not a linear relationship between the features and the actual probability — it's the logit that must be linear.\n",
        "\n",
        "2. Independence of Observations\n",
        "Each observation (data point) is assumed to be independent of the others.\n",
        "\n",
        "Violations occur in clustered or time-series data unless accounted for.\n",
        "\n",
        "3. Low or No Multicollinearity\n",
        "Predictor variables shouldn't be highly correlated with each other.\n",
        "\n",
        "Multicollinearity can distort the effect of each predictor and make coefficient estimates unreliable.\n",
        "\n",
        "4. No Outliers with Undue Influence\n",
        "Extreme values can skew the model, especially in small datasets.\n",
        "\n",
        "Use diagnostics like Cook’s distance to identify them.\n",
        "\n",
        "5. Sufficiently Large Sample Size\n",
        "Logistic regression needs a decent number of observations, especially when the outcome is rare.\n",
        "\n",
        "Rule of thumb: at least 10 events per predictor variable.\n",
        "\n",
        "6. Binary or Dichotomous Outcome (for binary logistic)\n",
        "The dependent variable must be binary (e.g., 0/1, yes/no).\n",
        "\n",
        "For multi-class outcomes, you'd use multinomial logistic regression.\n",
        "\n",
        "**10.  What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "Here are some of the main alternatives to logistic regression for classification:\n",
        "\n",
        "✅ 1. Decision Trees\n",
        "What it is: A tree-like structure where each node represents a decision based on a feature, and the branches represent outcomes.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Handles non-linear relationships well.\n",
        "\n",
        "Easy to interpret.\n",
        "\n",
        "Can capture interactions between features.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Prone to overfitting.\n",
        "\n",
        "Sensitive to small changes in data (unstable).\n",
        "\n",
        "✅ 2. Random Forest\n",
        "What it is: An ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Generally more accurate than individual decision trees.\n",
        "\n",
        "Robust to overfitting due to averaging over multiple trees.\n",
        "\n",
        "Handles high-dimensional data well.\n",
        "\n",
        "Cons:\n",
        "\n",
        "More complex and less interpretable.\n",
        "\n",
        "Can be computationally expensive.\n",
        "\n",
        "✅ 3. Support Vector Machines (SVM)\n",
        "What it is: A classifier that finds a hyperplane that best separates different classes in the feature space.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Effective in high-dimensional spaces.\n",
        "\n",
        "Works well for both linear and non-linear classification (with kernel trick).\n",
        "\n",
        "Cons:\n",
        "\n",
        "Memory-intensive.\n",
        "\n",
        "Sensitive to choice of kernel and hyperparameters.\n",
        "\n",
        "✅ 4. K-Nearest Neighbors (KNN)\n",
        "What it is: A non-parametric method where classification is based on the majority class of the nearest neighbors to the input data point.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Simple to understand and implement.\n",
        "\n",
        "No training phase, making it a \"lazy learner.\"\n",
        "\n",
        "Cons:\n",
        "\n",
        "Computationally expensive during prediction (as it requires calculating distances for each point).\n",
        "\n",
        "Sensitive to irrelevant features (feature scaling is important).\n",
        "\n",
        "✅ 5. Naive Bayes\n",
        "What it is: A probabilistic classifier based on Bayes' theorem, assuming that features are conditionally independent.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Simple, fast, and effective, especially for text classification (e.g., spam filtering).\n",
        "\n",
        "Performs well even with relatively small datasets.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Assumption of feature independence is often unrealistic in practice.\n",
        "\n",
        "Struggles with highly correlated features.\n",
        "\n",
        "✅ 6. Gradient Boosting Machines (GBM)\n",
        "What it is: An ensemble technique that builds trees sequentially, where each new tree tries to correct the errors made by the previous ones.\n",
        "\n",
        "Variants:\n",
        "\n",
        "XGBoost, LightGBM, and CatBoost are popular implementations.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Often provides state-of-the-art results.\n",
        "\n",
        "Can handle both linear and non-linear data patterns.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Tends to be slow to train.\n",
        "\n",
        "Requires careful tuning of hyperparameters.\n",
        "\n",
        "✅ 7. Neural Networks (Deep Learning)\n",
        "What it is: A network of interconnected layers of neurons, capable of modeling complex, non-linear relationships.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Highly flexible and capable of handling very complex data (e.g., images, text).\n",
        "\n",
        "Can model intricate patterns that other algorithms can't capture.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Requires large datasets and computational resources.\n",
        "\n",
        "Can be challenging to interpret.\n",
        "\n",
        "Prone to overfitting without regularization techniques.\n",
        "\n",
        "✅ 8. Linear Discriminant Analysis (LDA)\n",
        "What it is: A classifier that assumes data points are normally distributed and tries to find a linear combination of features that best separates classes.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Effective when features are highly correlated.\n",
        "\n",
        "Works well for small datasets when class distributions are normal.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Assumptions about normality and equal class covariances may not hold in practice.\n",
        "\n",
        "Not as flexible as other models like decision trees.\n",
        "\n",
        "✅ 9. Adaptive Boosting (AdaBoost)\n",
        "What it is: An ensemble method that combines weak classifiers (like decision trees) and focuses on examples that previous classifiers got wrong.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Can improve performance significantly.\n",
        "\n",
        "Less prone to overfitting compared to other boosting methods.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Sensitive to noisy data and outliers.\n",
        "\n",
        "**11.  What are Classification Evaluation Metrics?**\n",
        "\n",
        "In classification problems, evaluation metrics are essential for understanding how well the model is performing. These metrics help you assess different aspects of the model, such as its ability to correctly identify the positive and negative classes, handle imbalanced datasets, and minimize errors.\n",
        "\n",
        "Here’s a breakdown of some key classification evaluation metrics:\n",
        "\n",
        "✅ 1. Accuracy\n",
        "Definition: The proportion of correctly classified instances to the total instances.\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "True Positives\n",
        "+\n",
        "True Negatives\n",
        "Total Instances\n",
        "Accuracy=\n",
        "Total Instances\n",
        "True Positives+True Negatives\n",
        "​\n",
        "\n",
        "When to use: When the class distribution is balanced (i.e., both classes are approximately equal in number).\n",
        "\n",
        "Limitations: Accuracy can be misleading when classes are imbalanced (e.g., if 95% of instances belong to one class, a model predicting that class all the time would have 95% accuracy but perform poorly in practice).\n",
        "\n",
        "✅ 2. Precision (Positive Predictive Value)\n",
        "Definition: The proportion of true positives (correctly predicted positive cases) out of all predicted positives.\n",
        "\n",
        "Precision\n",
        "=\n",
        "True Positives\n",
        "True Positives\n",
        "+\n",
        "False Positives\n",
        "Precision=\n",
        "True Positives+False Positives\n",
        "True Positives\n",
        "​\n",
        "\n",
        "When to use: When false positives are costly or undesirable (e.g., in medical diagnoses where you don’t want to falsely diagnose someone as sick).\n",
        "\n",
        "Interpretation: Precision tells you how accurate your positive predictions are.\n",
        "\n",
        "✅ 3. Recall (Sensitivity or True Positive Rate)\n",
        "Definition: The proportion of true positives out of all actual positive cases.\n",
        "\n",
        "Recall\n",
        "=\n",
        "True Positives\n",
        "True Positives\n",
        "+\n",
        "False Negatives\n",
        "Recall=\n",
        "True Positives+False Negatives\n",
        "True Positives\n",
        "​\n",
        "\n",
        "When to use: When false negatives are costly or undesirable (e.g., in fraud detection where failing to identify fraud is worse than falsely identifying a case).\n",
        "\n",
        "Interpretation: Recall tells you how well the model identifies all relevant instances (i.e., it captures as many positive cases as possible).\n",
        "\n",
        "✅ 4. F1-Score\n",
        "Definition: The harmonic mean of precision and recall, giving a balance between them.\n",
        "\n",
        "𝐹\n",
        "1\n",
        "=\n",
        "2\n",
        "⋅\n",
        "Precision\n",
        "⋅\n",
        "Recall\n",
        "Precision\n",
        "+\n",
        "Recall\n",
        "F1=2⋅\n",
        "Precision+Recall\n",
        "Precision⋅Recall\n",
        "​\n",
        "\n",
        "When to use: When you need a balance between precision and recall, especially in cases where there is an imbalance between classes.\n",
        "\n",
        "Interpretation: F1-score provides a single metric that balances the trade-off between precision and recall. It is especially useful when both false positives and false negatives are important.\n",
        "\n",
        "✅ 5. Specificity (True Negative Rate)\n",
        "Definition: The proportion of true negatives (correctly predicted negative cases) out of all actual negative cases.\n",
        "\n",
        "Specificity\n",
        "=\n",
        "True Negatives\n",
        "True Negatives\n",
        "+\n",
        "False Positives\n",
        "Specificity=\n",
        "True Negatives+False Positives\n",
        "True Negatives\n",
        "​\n",
        "\n",
        "When to use: When false positives are costly or you want to focus on how well the model avoids false positives.\n",
        "\n",
        "Interpretation: Specificity is a measure of how well the model identifies negative cases.\n",
        "\n",
        "✅ 6. ROC Curve (Receiver Operating Characteristic Curve)\n",
        "Definition: A graphical representation of the trade-off between true positive rate (recall) and false positive rate at different classification thresholds.\n",
        "\n",
        "Interpretation: The curve shows the performance of a classifier at all thresholds. The area under the ROC curve (AUC-ROC) gives a single number that summarizes the performance.\n",
        "\n",
        "AUC = 0.5 means random performance.\n",
        "\n",
        "AUC = 1 means perfect performance.\n",
        "\n",
        "✅ 7. Area Under the ROC Curve (AUC-ROC)\n",
        "Definition: A summary statistic that quantifies the overall ability of the model to distinguish between the positive and negative classes. It measures the area under the ROC curve.\n",
        "\n",
        "When to use: When you want to evaluate the overall performance of the model, especially for imbalanced datasets.\n",
        "\n",
        "Interpretation: Higher AUC indicates better model performance.\n",
        "\n",
        "✅ 8. Confusion Matrix\n",
        "Definition: A matrix that summarizes the performance of the classification model by showing the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
        "\n",
        "\n",
        "Predicted Positive\tPredicted Negative\n",
        "Actual Positive\tTrue Positives (TP)\tFalse Negatives (FN)\n",
        "Actual Negative\tFalse Positives (FP)\tTrue Negatives (TN)\n",
        "When to use: Provides a detailed view of model performance and can be used to calculate other metrics (precision, recall, etc.).\n",
        "\n",
        "✅ 9. Log Loss (Cross-Entropy Loss)\n",
        "Definition: Measures the performance of a classification model whose output is a probability value between 0 and 1. Log loss calculates the penalty for incorrect predictions.\n",
        "\n",
        "Log Loss\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "Log Loss=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(p\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−p\n",
        "(i)\n",
        " )]\n",
        "Where\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "p\n",
        "(i)\n",
        "  is the predicted probability and\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  is the actual label.\n",
        "\n",
        "When to use: When your model outputs probabilities and you want to evaluate the accuracy of probability estimates.\n",
        "\n",
        "\n",
        "**12. How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "Class imbalance can affect logistic regression in several ways:\n",
        "\n",
        "✅ Effects of Class Imbalance on Logistic Regression:\n",
        "Biased Predictions Toward the Majority Class:\n",
        "\n",
        "Logistic regression models are trained to minimize the overall logistic loss (cross-entropy loss), which may lead to a bias toward predicting the majority class.\n",
        "\n",
        "For example, if 95% of the data points belong to the negative class (0), the model might predict 0 for most cases, because predicting 0 most of the time would still result in a high accuracy (but low performance for detecting the minority class).\n",
        "\n",
        "Poor Performance on the Minority Class:\n",
        "\n",
        "The model might fail to capture the characteristics of the minority class (e.g., fraudulent transactions, rare diseases, etc.). As a result, the minority class (positive class) will often be misclassified as the majority class (negative class), leading to low recall for the minority class.\n",
        "\n",
        "Skewed Evaluation Metrics:\n",
        "\n",
        "With a highly imbalanced dataset, traditional evaluation metrics like accuracy are not very informative.\n",
        "\n",
        "A model with 95% accuracy by predicting the majority class all the time can perform poorly on the minority class.\n",
        "\n",
        "Metrics like precision, recall, F1-score, and AUC-ROC are more useful when dealing with class imbalance.\n",
        "\n",
        "**13. What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "Hyperparameter tuning refers to the process of selecting the optimal set of hyperparameters for a machine learning model to achieve the best performance. In the case of logistic regression, hyperparameter tuning is crucial to improve the model's accuracy, prevent overfitting or underfitting, and ensure generalization to new, unseen data.\n",
        "\n",
        "**14.  What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "In Logistic Regression, solvers are algorithms used to find the optimal parameters (weights) by minimizing the cost function. Different solvers use different optimization techniques and are suited for different types of problems, including variations in data size, the number of features, and the choice of regularization.\n",
        "\n",
        "Here are the different solvers available in logistic regression and their characteristics:\n",
        "\n",
        "1. liblinear (Coordinate Descent Algorithm)\n",
        "Description: This solver uses coordinate descent, an iterative algorithm that minimizes the cost function by updating one coefficient at a time.\n",
        "\n",
        "Features:\n",
        "\n",
        "Works well for small to medium-sized datasets.\n",
        "\n",
        "Supports L1 (Lasso) regularization, so it's useful when you want to perform feature selection (sparse models).\n",
        "\n",
        "Can handle both binary classification and multiclass classification (via a one-vs-rest approach).\n",
        "\n",
        "Slower on large datasets compared to other solvers like saga or lbfgs.\n",
        "\n",
        "When to use:\n",
        "\n",
        "If you have a small dataset and want to use L1 regularization.\n",
        "\n",
        "When your problem requires sparse solutions (many coefficients equal to zero).\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "2. newton-cg (Newton's Method with Conjugate Gradient)\n",
        "Description: This solver uses Newton's method, an optimization technique that makes use of second-order derivatives (Hessian matrix) to update parameters. The conjugate gradient method is used to handle the large Hessian in logistic regression.\n",
        "\n",
        "Features:\n",
        "\n",
        "Works well for medium to large datasets.\n",
        "\n",
        "Supports L2 regularization only.\n",
        "\n",
        "Faster than liblinear for large datasets.\n",
        "\n",
        "More efficient for multiclass classification.\n",
        "\n",
        "Uses more memory because it computes second-order derivatives.\n",
        "\n",
        "When to use:\n",
        "\n",
        "If you have a medium-sized dataset and prefer L2 regularization.\n",
        "\n",
        "When you need efficient optimization for multiclass classification.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='newton-cg')\n",
        "3. lbfgs (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n",
        "Description: The LBFGS algorithm is a quasi-Newton method that approximates the Hessian matrix, making it memory-efficient for large datasets. It is based on Broyden-Fletcher-Goldfarb-Shanno (BFGS) and is used for optimizing smooth, continuous functions.\n",
        "\n",
        "Features:\n",
        "\n",
        "Works well for large datasets.\n",
        "\n",
        "Supports L2 regularization only.\n",
        "\n",
        "Faster and more memory-efficient than newton-cg for large datasets.\n",
        "\n",
        "Does not handle L1 regularization.\n",
        "\n",
        "When to use:\n",
        "\n",
        "If you have a large dataset with L2 regularization.\n",
        "\n",
        "When you want efficient handling of multiclass classification.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "4. saga (Stochastic Average Gradient Descent)\n",
        "Description: The SAGA algorithm is a variant of stochastic gradient descent (SGD) that improves convergence speed and stability, especially when dealing with large datasets and regularization. It's an efficient solver for both L1 and L2 regularization, and supports Elastic Net regularization.\n",
        "\n",
        "Features:\n",
        "\n",
        "Works well for large datasets.\n",
        "\n",
        "Supports L1, L2, and Elastic Net regularization.\n",
        "\n",
        "Can handle multiclass classification using a one-vs-rest approach.\n",
        "\n",
        "Faster convergence compared to traditional SGD.\n",
        "\n",
        "When to use:\n",
        "\n",
        "If you have a large dataset and want to use L1, L2, or Elastic Net regularization.\n",
        "\n",
        "When you need regularization flexibility (such as combining L1 and L2 regularization).\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='saga')\n",
        "5. sgd (Stochastic Gradient Descent)\n",
        "Description: This solver uses stochastic gradient descent, an optimization method where the model is updated incrementally by processing one data point at a time.\n",
        "\n",
        "Features:\n",
        "\n",
        "Works well for very large datasets.\n",
        "\n",
        "Requires careful tuning of the learning rate.\n",
        "\n",
        "Supports L1 and L2 regularization, and can also be used with Elastic Net.\n",
        "\n",
        "Slower convergence, requires more iterations and tuning compared to other solvers.\n",
        "\n",
        "When to use:\n",
        "\n",
        "If you have a very large dataset.\n",
        "\n",
        "When you want to apply L1, L2, or Elastic Net regularization with stochastic updates.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='sgd')\n",
        "⚖️ Which Solver to Use?\n",
        "\n",
        "Solver\tBest For\tRegularization Supported\tDataset Size\tUsage\n",
        "liblinear\tSmall datasets, L1 regularization\tL1 and L2\tSmall\tWhen using L1 regularization and small data\n",
        "newton-cg\tMedium to large datasets, L2 regularization\tL2\tMedium-Large\tWhen using L2 regularization and multiclass\n",
        "lbfgs\tLarge datasets, L2 regularization\tL2\tLarge\tWhen using L2 regularization and large data\n",
        "saga\tLarge datasets, L1 or Elastic Net regularization\tL1, L2, Elastic Net\tLarge\tWhen using L1 or Elastic Net regularization\n",
        "sgd\tVery large datasets, L1, L2, or Elastic Net\tL1, L2, Elastic Net\tVery Large\tWhen using L1, L2, or Elastic Net with very large datasets\n",
        "\n",
        "\n",
        "**15. How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "Logistic Regression is inherently a binary classification algorithm, meaning it can only classify instances into two classes. However, it can be extended to handle multiclass classification (where the target variable has more than two classes) using two main strategies:\n",
        "\n",
        "One-vs-Rest (OvR), also called One-vs-All (OvA)\n",
        "\n",
        "Multinomial Logistic Regression\n",
        "\n",
        "1. One-vs-Rest (OvR) / One-vs-All (OvA)\n",
        "In this approach, the idea is to train one binary classifier for each class. For a classification problem with\n",
        "𝑘\n",
        "k classes, you train\n",
        "𝑘\n",
        "k separate binary classifiers, where each classifier distinguishes one class from the others.\n",
        "\n",
        "For each classifier:\n",
        "\n",
        "Treat the target class as the positive class and all other classes as the negative class.\n",
        "\n",
        "The output of each classifier is a probability that the instance belongs to that class.\n",
        "\n",
        "During prediction:\n",
        "\n",
        "Each classifier will predict a probability for its associated class.\n",
        "\n",
        "The final prediction is the class corresponding to the classifier with the highest probability.\n",
        "\n",
        "How it works:\n",
        "\n",
        "If you have 3 classes: A, B, and C, the classifier will:\n",
        "\n",
        "Classifier 1: Classify A vs. not A (i.e., classes B and C).\n",
        "\n",
        "Classifier 2: Classify B vs. not B (i.e., classes A and C).\n",
        "\n",
        "Classifier 3: Classify C vs. not C (i.e., classes A and B).\n",
        "\n",
        "Each classifier produces a probability score for the instance belonging to its respective class, and the instance is assigned to the class with the highest score.\n",
        "\n",
        "\n",
        "2. Multinomial Logistic Regression (Softmax Regression)\n",
        "\n",
        "In multinomial logistic regression, also known as softmax regression, the model is extended to directly predict the probabilities for multiple classes by using the softmax function.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Instead of training separate binary classifiers, multinomial logistic regression uses a single model to predict the probabilities of each class.\n",
        "\n",
        "The softmax function converts the raw output of the linear decision function (also known as logits) into a probability distribution over all possible classes.\n",
        "\n",
        "The softmax function ensures that the sum of the probabilities for all classes is equal to 1. Given a set of class scores\n",
        "𝑧\n",
        "1\n",
        ",\n",
        "𝑧\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑧\n",
        "𝑘\n",
        "z\n",
        "1\n",
        "​\n",
        " ,z\n",
        "2\n",
        "​\n",
        " ,...,z\n",
        "k\n",
        "​\n",
        "  for each class, the softmax function is defined as:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑗\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑧\n",
        "𝑗\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑒\n",
        "𝑧\n",
        "𝑖\n",
        "P(y=j∣x)=\n",
        "∑\n",
        "i=1\n",
        "k\n",
        "​\n",
        " e\n",
        "z\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "e\n",
        "z\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "\n",
        "**16. What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "Advantages of Logistic Regression:\n",
        "Simplicity and Interpretability:\n",
        "\n",
        "Logistic regression is one of the simplest machine learning algorithms. It provides clear interpretations of model parameters (coefficients), which makes it easy to understand how the model is making predictions.\n",
        "\n",
        "The coefficients tell you how much each feature contributes to the prediction, making it easy to interpret and explain.\n",
        "\n",
        "Efficiency and Speed:\n",
        "\n",
        "It is computationally efficient, especially for smaller datasets. Logistic regression has a low memory footprint and can be trained quickly compared to more complex models like decision trees, random forests, or neural networks.\n",
        "\n",
        "It converges relatively fast and does not require a lot of tuning.\n",
        "\n",
        "Probabilistic Output:\n",
        "\n",
        "Logistic regression outputs probabilities, which gives not only the predicted class but also the confidence in that prediction.\n",
        "\n",
        "The probability output can be useful for applications where you need to assess the likelihood of a class and decide based on thresholds.\n",
        "\n",
        "Works Well for Linearly Separable Data:\n",
        "\n",
        "It performs well when the data is linearly separable or approximately linear, meaning the classes can be separated by a straight line (or hyperplane in higher dimensions).\n",
        "\n",
        "Less Prone to Overfitting (with Regularization):\n",
        "\n",
        "Logistic regression can be regularized (using L1 or L2 regularization) to prevent overfitting, making it more robust on small datasets or datasets with many features.\n",
        "\n",
        "Versatility in Applications:\n",
        "\n",
        "Logistic regression is commonly used in binary classification tasks (e.g., spam detection, fraud detection), but can be extended to multiclass classification (e.g., with the One-vs-Rest or multinomial strategies).\n",
        "\n",
        "Less Memory Intensive:\n",
        "\n",
        "Logistic regression has low memory requirements compared to more complex models like deep neural networks or ensemble methods.\n",
        "\n",
        "Disadvantages of Logistic Regression:\n",
        "Assumes Linearity:\n",
        "\n",
        "Logistic regression assumes a linear relationship between the features and the log-odds of the target variable. If the data has complex, non-linear relationships, logistic regression might not perform well.\n",
        "\n",
        "This makes it less suitable for non-linear decision boundaries (e.g., in image or speech recognition).\n",
        "\n",
        "Sensitive to Outliers:\n",
        "\n",
        "Logistic regression can be sensitive to outliers in the data, which may distort the decision boundary and lead to poor performance. Outliers can affect the estimated parameters significantly.\n",
        "\n",
        "Limited to Linear Decision Boundaries (without transformations):\n",
        "\n",
        "In its basic form, logistic regression only draws linear decision boundaries. If the underlying data requires a more complex boundary (non-linear), logistic regression without feature transformations may not perform well.\n",
        "\n",
        "Performance Issues on Highly Correlated Data:\n",
        "\n",
        "Logistic regression assumes that the features are independent. When features are highly correlated, this can lead to multicollinearity, making the model parameters unstable and difficult to interpret.\n",
        "\n",
        "Regularization can help reduce the impact of multicollinearity, but it may not always be sufficient.\n",
        "\n",
        "Not Ideal for Complex Relationships:\n",
        "\n",
        "While logistic regression can perform well in certain scenarios, it does not capture complex interactions between features unless you explicitly include them as features (e.g., polynomial features).\n",
        "\n",
        "For complex data with high-order interactions or non-linear relationships, more sophisticated models (like decision trees, SVM, or neural networks) might perform better.\n",
        "\n",
        "Assumes Homoscedasticity:\n",
        "\n",
        "Logistic regression assumes that the variance of errors is the same for all observations (homoscedasticity). If this assumption is violated (heteroscedasticity), it can impact model performance.\n",
        "\n",
        "May Struggle with Large Datasets or High Dimensionality:\n",
        "\n",
        "Although logistic regression can handle a reasonable amount of features, it may struggle with high-dimensional data (many features) without proper feature selection or regularization.\n",
        "\n",
        "If the number of features is large, it may require feature engineering (e.g., dimensionality reduction) to improve performance.\n",
        "\n",
        "Requires a Good Choice of Features:\n",
        "\n",
        "Logistic regression performs poorly if the relevant features are not included in the model or if irrelevant features are included. It's crucial to have a good set of features for the model to perform well.\n",
        "\n",
        "**17. What are some use cases of Logistic Regression?**\n",
        "\n",
        "Logistic Regression is a versatile and widely used algorithm, particularly for binary classification tasks. Its ability to output probabilities and the simplicity of its model make it useful in a wide variety of applications. Below are some common use cases of Logistic Regression:\n",
        "\n",
        "1. Spam Email Detection\n",
        "Use Case: Classifying emails as either spam or not spam.\n",
        "\n",
        "How it Works: The algorithm learns from various features of the email (e.g., keywords, sender, frequency of certain words) to classify new emails.\n",
        "\n",
        "Why Logistic Regression: Emails are typically binary (spam or not), and logistic regression can output the probability that an email is spam, which is helpful for sorting.\n",
        "\n",
        "2. Medical Diagnosis (Disease Prediction)\n",
        "Use Case: Predicting whether a patient has a certain disease based on various symptoms, test results, and patient characteristics.\n",
        "\n",
        "Example: Predicting heart disease or diabetes from medical records, such as blood pressure, cholesterol levels, age, and family history.\n",
        "\n",
        "Why Logistic Regression: Logistic regression is simple and provides probabilities that can be used for medical decision-making. It is suitable for predicting outcomes where there are clear diagnostic features.\n",
        "\n",
        "3. Credit Scoring and Loan Default Prediction\n",
        "Use Case: Predicting whether a person will default on a loan or if they will be able to repay the loan.\n",
        "\n",
        "How it Works: Features such as income, credit history, loan amount, and other financial factors are used to predict the probability of default.\n",
        "\n",
        "Why Logistic Regression: Credit scoring is essentially a binary classification problem (default vs. no default), and logistic regression helps estimate the likelihood of each outcome.\n",
        "\n",
        "4. Customer Churn Prediction\n",
        "Use Case: Predicting whether a customer will leave or stay with a service (e.g., telecom, subscription-based services, or utilities).\n",
        "\n",
        "How it Works: The algorithm uses customer behavior data (e.g., usage patterns, service calls, payment history) to predict the likelihood that a customer will churn (leave the service).\n",
        "\n",
        "Why Logistic Regression: It's a common binary classification problem (churn vs. no churn), and logistic regression can help businesses take preventive action based on the probability of churn.\n",
        "\n",
        "5. Fraud Detection\n",
        "Use Case: Detecting fraudulent transactions or behavior, such as credit card fraud or insurance fraud.\n",
        "\n",
        "How it Works: The model is trained on historical transaction data and can predict whether a new transaction is likely fraudulent based on features such as transaction amount, location, and user behavior.\n",
        "\n",
        "Why Logistic Regression: Fraud detection is a binary classification problem, and logistic regression provides useful probabilities to flag suspicious activities.\n",
        "\n",
        "6. Marketing and Targeted Advertising\n",
        "Use Case: Predicting the likelihood of a customer clicking on an ad or responding to a marketing campaign.\n",
        "\n",
        "How it Works: Based on customer demographics, browsing history, and engagement with previous ads, logistic regression can predict whether a customer will take the desired action (e.g., click on an ad or make a purchase).\n",
        "\n",
        "Why Logistic Regression: It’s a binary classification problem (clicked vs. not clicked), and logistic regression is well-suited for this type of prediction.\n",
        "\n",
        "7. Sentiment Analysis\n",
        "Use Case: Classifying the sentiment of a text (e.g., positive or negative reviews).\n",
        "\n",
        "How it Works: The text is converted into numerical features (e.g., word frequency or TF-IDF), and logistic regression is used to classify the sentiment as positive or negative.\n",
        "\n",
        "Why Logistic Regression: It is effective for binary classification tasks like sentiment analysis, and logistic regression can help evaluate the likelihood of positive or negative sentiment.\n",
        "\n",
        "8. Image Classification (Binary Class)\n",
        "Use Case: Classifying images into two categories, such as cat vs. dog or healthy vs. diseased tissue in medical imaging.\n",
        "\n",
        "How it Works: Features from the images are extracted (e.g., pixel values, histograms), and logistic regression classifies the images into one of two classes.\n",
        "\n",
        "Why Logistic Regression: It works well for binary image classification, especially when the dataset is small or when the features can be linearly separated.\n",
        "\n",
        "9. Defaulting on Online Subscriptions\n",
        "Use Case: Predicting whether a user will cancel a subscription (e.g., streaming service, news website).\n",
        "\n",
        "How it Works: Features such as frequency of use, login activity, engagement with the service, and payment history are used to predict the likelihood of a user canceling the subscription.\n",
        "\n",
        "Why Logistic Regression: It's a binary classification task (cancel vs. stay), and logistic regression can offer a clear understanding of the likelihood of cancellation.\n",
        "\n",
        "10. Risk Prediction in Financial Markets\n",
        "Use Case: Predicting the likelihood of a stock price increase or decrease based on various market factors.\n",
        "\n",
        "How it Works: Logistic regression can be used to predict whether a stock will go up or down based on technical indicators like moving averages, market sentiment, and news articles.\n",
        "\n",
        "Why Logistic Regression: It's well-suited for predicting binary outcomes in financial markets (price increase vs. price decrease).\n",
        "\n",
        "11. Social Media Engagement Prediction\n",
        "Use Case: Predicting whether a post on social media will go viral or receive a certain level of engagement.\n",
        "\n",
        "How it Works: Logistic regression can use features like the content of the post, time of posting, and user demographics to predict whether the post will receive significant likes, shares, or comments.\n",
        "\n",
        "Why Logistic Regression: The task is binary (viral or not viral), and logistic regression is efficient in estimating probabilities for social media engagement.\n",
        "\n",
        "12. Employee Attrition Prediction\n",
        "Use Case: Predicting whether an employee will leave the company (attrition) or stay.\n",
        "\n",
        "How it Works: Logistic regression uses features like employee demographics, job satisfaction, salary, performance ratings, and tenure to predict the probability of an employee leaving.\n",
        "\n",
        "Why Logistic Regression: It's a binary classification problem, and logistic regression can predict the likelihood of attrition, helping HR take proactive measures.\n",
        "\n",
        "**18. What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "Logistic Regression and Softmax Regression are both linear classification models, but they differ primarily in how they handle the target variable (the class label) and the number of classes they are designed to handle. Here's a breakdown of the key differences between the two:\n",
        "\n",
        "1. Number of Classes\n",
        "Logistic Regression (Binary Logistic Regression):\n",
        "\n",
        "Designed for binary classification, i.e., when the target variable has only two classes (e.g., 0 vs 1, spam vs not spam).\n",
        "\n",
        "It predicts the probability that an instance belongs to one class (e.g., class 1) and implicitly assigns the other class (e.g., class 0).\n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression):\n",
        "\n",
        "Designed for multiclass classification, i.e., when the target variable has more than two classes (e.g., class 1, class 2, class 3, etc.).\n",
        "\n",
        "It directly predicts the probability distribution across all possible classes, ensuring the sum of probabilities for all classes is 1.\n",
        "\n",
        "2. Output\n",
        "Logistic Regression:\n",
        "\n",
        "Outputs a single probability between 0 and 1 that an instance belongs to the positive class (class 1).\n",
        "\n",
        "The probability for class 0 is implicitly given by\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        ")\n",
        "1−P(y=1).\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Outputs a probability for each class. For\n",
        "𝑘\n",
        "k classes, it provides\n",
        "𝑘\n",
        "k probabilities, each corresponding to one of the classes.\n",
        "\n",
        "The probabilities sum to 1, meaning the output is a probability distribution across all classes.\n",
        "\n",
        "3. Decision Function\n",
        "Logistic Regression:\n",
        "\n",
        "In binary logistic regression, the decision rule is simple:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(w⋅x+b)\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "The model predicts the class 1 if the probability is greater than 0.5, otherwise class 0.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "For multinomial logistic regression (softmax), the decision rule is:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑗\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑤\n",
        "𝑗\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "𝑗\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑒\n",
        "𝑤\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "𝑖\n",
        "P(y=j∣x)=\n",
        "∑\n",
        "i=1\n",
        "k\n",
        "​\n",
        " e\n",
        "w\n",
        "i\n",
        "​\n",
        " ⋅x+b\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "e\n",
        "w\n",
        "j\n",
        "​\n",
        " ⋅x+b\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "This function computes the probabilities for each class\n",
        "𝑗\n",
        "j using the exponential of the weighted sum for each class, and the denominator ensures that all probabilities sum to 1.\n",
        "\n",
        "4. Cost Function (Loss Function)\n",
        "Logistic Regression:\n",
        "\n",
        "The cost function for binary logistic regression is the binary cross-entropy (log loss):\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "It compares the predicted probability for class 1 against the actual class labels.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "The cost function for softmax regression is the categorical cross-entropy:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑦\n",
        "𝑗\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "𝑗\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "∣\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        " y\n",
        "j\n",
        "(i)\n",
        "​\n",
        " log(P(y\n",
        "j\n",
        "(i)\n",
        "​\n",
        " ∣x\n",
        "(i)\n",
        " ))\n",
        "This function compares the predicted probabilities across all classes against the actual class labels for multiclass classification.\n",
        "\n",
        "5. Use Cases\n",
        "Logistic Regression:\n",
        "\n",
        "Binary classification tasks, where the goal is to classify data into two categories.\n",
        "\n",
        "Examples: Predicting spam vs non-spam emails, fraudulent vs non-fraudulent transactions, disease vs no disease.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Multiclass classification tasks, where the goal is to classify data into more than two categories.\n",
        "\n",
        "Examples: Image classification (e.g., classifying animals into cat, dog, or bird), digit recognition (e.g., classifying digits 0-9), topic classification for articles (e.g., sports, politics, technology, etc.).\n",
        "\n",
        "6. Model Complexity\n",
        "Logistic Regression:\n",
        "\n",
        "Simpler, as it only handles two classes. It can be thought of as a single model that tries to separate two classes.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "More complex, as it handles multiple classes simultaneously. It needs to model multiple decision boundaries and calculate probabilities for each class.\n",
        "\n",
        "**19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "When you're working on multiclass classification, choosing between One-vs-Rest (OvR) (also called One-vs-All) and Softmax regression depends on various factors including the nature of the problem, the performance considerations, and the model's complexity. Both approaches are designed to handle multiclass problems, but they work differently.\n",
        "\n",
        "Here's a breakdown of the two methods and guidance on how to choose between them:\n",
        "\n",
        "1. One-vs-Rest (OvR) (One-vs-All)\n",
        "Concept:\n",
        "\n",
        "The One-vs-Rest (OvR) approach involves training a binary classifier for each class, where each classifier learns to distinguish one class from all other classes.\n",
        "\n",
        "For k classes, k binary classifiers are trained. Each classifier is trained on the data where it labels the target class as positive and all other classes as negative.\n",
        "\n",
        "When making predictions, the class that has the highest predicted probability (or score) is chosen as the output.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "Suppose we have 3 classes: A, B, and C.\n",
        "\n",
        "Classifier 1: Trains on data labeled as A vs not A (B, C).\n",
        "\n",
        "Classifier 2: Trains on data labeled as B vs not B (A, C).\n",
        "\n",
        "Classifier 3: Trains on data labeled as C vs not C (A, B).\n",
        "\n",
        "During prediction, you get 3 separate probabilities (one from each classifier), and the highest probability determines the predicted class.\n",
        "\n",
        "Advantages of OvR:\n",
        "\n",
        "Simple and Efficient: It's easier to implement and computationally less demanding when using models that are already binary classifiers (like logistic regression).\n",
        "\n",
        "Scalable: Can handle cases where there are many classes, as long as you can train each binary classifier independently.\n",
        "\n",
        "More Flexibility: You can use different classifiers for each class if needed (e.g., you can mix logistic regression, decision trees, etc., for each class).\n",
        "\n",
        "Disadvantages of OvR:\n",
        "\n",
        "Inefficiency in Handling Non-Linearly Separable Classes: If classes are not linearly separable, OvR might struggle since each classifier only sees two classes at a time.\n",
        "\n",
        "Inconsistent Decision Boundaries: Each binary classifier makes its decision independently, which may lead to inconsistent decision boundaries when classes are not well-separated.\n",
        "\n",
        "Overfitting Risk: Since you're training multiple classifiers, overfitting can occur if not properly regularized.\n",
        "\n",
        "2. Softmax Regression (Multinomial Logistic Regression)\n",
        "Concept:\n",
        "\n",
        "Softmax Regression is a single, multiclass classifier that treats all classes simultaneously. It outputs a probability distribution across all classes using a single model.\n",
        "\n",
        "Instead of training multiple binary classifiers, Softmax regression treats the problem as a multinomial classification task, where the model computes the probabilities for each class and chooses the one with the highest probability.\n",
        "\n",
        "The Softmax function ensures that the sum of probabilities across all classes is 1, and each class is treated with equal importance.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "The Softmax function calculates the probabilities for each class\n",
        "𝑘\n",
        "k as:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑤\n",
        "𝑘\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "𝑘\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝑤\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "𝑖\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "i=1\n",
        "K\n",
        "​\n",
        " e\n",
        "w\n",
        "i\n",
        "​\n",
        " ⋅x+b\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "e\n",
        "w\n",
        "k\n",
        "​\n",
        " ⋅x+b\n",
        "k\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "where\n",
        "𝐾\n",
        "K is the number of classes,\n",
        "𝑤\n",
        "𝑘\n",
        "w\n",
        "k\n",
        "​\n",
        "  is the weight vector for class\n",
        "𝑘\n",
        "k, and\n",
        "𝑏\n",
        "𝑘\n",
        "b\n",
        "k\n",
        "​\n",
        "  is the bias for class\n",
        "𝑘\n",
        "k.\n",
        "\n",
        "This provides a probability distribution across all classes, and the class with the highest probability is chosen.\n",
        "\n",
        "Advantages of Softmax Regression:\n",
        "\n",
        "Consistent Decision Boundaries: Unlike OvR, Softmax regression makes all decisions simultaneously, so it produces more consistent and coherent decision boundaries.\n",
        "\n",
        "Better Handling of Class Imbalance: Softmax regression handles class imbalances more effectively since it takes into account all classes together.\n",
        "\n",
        "Single Model: Since it's a single model, you avoid the overhead of training multiple binary classifiers.\n",
        "\n",
        "Disadvantages of Softmax Regression:\n",
        "\n",
        "Less Flexible: Unlike OvR, where each classifier can be customized individually, Softmax regression uses a single model and is less flexible in terms of combining different algorithms.\n",
        "\n",
        "More Complex: Softmax regression is more computationally expensive compared to OvR, especially when the number of classes is large, since it requires training a single model with k classes.\n",
        "\n",
        "May Struggle with Highly Imbalanced Data: If there is a large imbalance in the classes, Softmax may not always perform well without regularization or additional techniques to handle the imbalance.\n",
        "\n",
        "**20. How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "Interpreting the coefficients in Logistic Regression is a crucial step to understand how the model makes its predictions and how each feature influences the likelihood of the outcome. Unlike Linear Regression, where the coefficients represent the direct change in the predicted value, the coefficients in Logistic Regression are related to the log-odds of the outcome.\n",
        "\n",
        "1. Logistic Regression Model Recap\n",
        "In Logistic Regression, the model predicts the probability of an event occurring (e.g., class 1) as:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑤\n",
        "0\n",
        "+\n",
        "𝑤\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑤\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝑤\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(w\n",
        "0\n",
        "​\n",
        " +w\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +w\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +...+w\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x) is the probability that the outcome is 1 (positive class).\n",
        "\n",
        "𝑤\n",
        "0\n",
        "w\n",
        "0\n",
        "​\n",
        "  is the intercept (bias).\n",
        "\n",
        "𝑤\n",
        "1\n",
        ",\n",
        "𝑤\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑤\n",
        "𝑛\n",
        "w\n",
        "1\n",
        "​\n",
        " ,w\n",
        "2\n",
        "​\n",
        " ,...,w\n",
        "n\n",
        "​\n",
        "  are the coefficients for each feature\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,...,x\n",
        "n\n",
        "​\n",
        " .\n",
        "\n",
        "The model outputs a probability between 0 and 1. The key here is that Logistic Regression uses the log-odds to predict this probability.\n",
        "\n",
        "2. Understanding the Coefficients\n",
        "The coefficients\n",
        "𝑤\n",
        "1\n",
        ",\n",
        "𝑤\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑤\n",
        "𝑛\n",
        "w\n",
        "1\n",
        "​\n",
        " ,w\n",
        "2\n",
        "​\n",
        " ,...,w\n",
        "n\n",
        "​\n",
        "  correspond to the log-odds of the outcome, not directly to the probability itself. To understand this better, let’s break down how to interpret them.\n",
        "\n",
        "Log-Odds Interpretation\n",
        "The logistic regression model can be expressed in terms of log-odds as:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "=\n",
        "𝑤\n",
        "0\n",
        "+\n",
        "𝑤\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑤\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝑤\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "P(y=0∣x)\n",
        "P(y=1∣x)\n",
        "​\n",
        " )=w\n",
        "0\n",
        "​\n",
        " +w\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +w\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +...+w\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "This means that the log of the odds of the outcome occurring (class 1) is a linear combination of the input features. The odds are the ratio of the probability of class 1 to the probability of class 0, i.e.,\n",
        "\n",
        "Odds\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "Odds=\n",
        "P(y=0∣x)\n",
        "P(y=1∣x)\n",
        "​\n",
        "\n",
        "Now, let’s understand the interpretation of the individual coefficients:\n",
        "\n",
        "Coefficient Interpretation:\n",
        "Each coefficient\n",
        "𝑤\n",
        "𝑖\n",
        "w\n",
        "i\n",
        "​\n",
        "  represents the change in the log-odds of the outcome (class 1) for a one-unit increase in the corresponding feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " , holding all other features constant.\n",
        "\n",
        "If\n",
        "𝑤\n",
        "𝑖\n",
        ">\n",
        "0\n",
        "w\n",
        "i\n",
        "​\n",
        " >0, an increase in\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  will increase the odds of class 1 (i.e., increase the probability of the positive outcome).\n",
        "\n",
        "If\n",
        "𝑤\n",
        "𝑖\n",
        "<\n",
        "0\n",
        "w\n",
        "i\n",
        "​\n",
        " <0, an increase in\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  will decrease the odds of class 1 (i.e., decrease the probability of the positive outcome).\n",
        "\n",
        "3. Converting Coefficients to Odds Ratios\n",
        "Since the coefficients represent the log-odds, it’s often easier to interpret them as odds ratios by exponentiating the coefficients. The odds ratio is the multiplicative change in the odds for a one-unit increase in the predictor\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " .\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝑤\n",
        "𝑖\n",
        "Odds Ratio=e\n",
        "w\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "Interpretation of Odds Ratio:\n",
        "If\n",
        "𝑒\n",
        "𝑤\n",
        "𝑖\n",
        ">\n",
        "1\n",
        "e\n",
        "w\n",
        "i\n",
        "​\n",
        "\n",
        " >1, the odds of the outcome (class 1) increase as\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases.\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝑤\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "e\n",
        "w\n",
        "i\n",
        "​\n",
        "\n",
        " =1, the odds of the outcome remain the same as\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases (i.e., no effect).\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝑤\n",
        "𝑖\n",
        "<\n",
        "1\n",
        "e\n",
        "w\n",
        "i\n",
        "​\n",
        "\n",
        " <1, the odds of the outcome decrease as\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases.\n",
        "\n",
        "For example:\n",
        "\n",
        "If\n",
        "𝑤\n",
        "1\n",
        "=\n",
        "0.5\n",
        "w\n",
        "1\n",
        "​\n",
        " =0.5, the odds ratio would be\n",
        "𝑒\n",
        "0.5\n",
        "≈\n",
        "1.65\n",
        "e\n",
        "0.5\n",
        " ≈1.65, which means that for every one-unit increase in\n",
        "𝑥\n",
        "1\n",
        "x\n",
        "1\n",
        "​\n",
        " , the odds of the outcome being class 1 increase by 65%.\n",
        "\n",
        "If\n",
        "𝑤\n",
        "2\n",
        "=\n",
        "−\n",
        "0.7\n",
        "w\n",
        "2\n",
        "​\n",
        " =−0.7, the odds ratio would be\n",
        "𝑒\n",
        "−\n",
        "0.7\n",
        "≈\n",
        "0.50\n",
        "e\n",
        "−0.7\n",
        " ≈0.50, which means that for every one-unit increase in\n",
        "𝑥\n",
        "2\n",
        "x\n",
        "2\n",
        "​\n",
        " , the odds of the outcome being class 1 decrease by 50%.\n",
        "\n",
        "4. Example\n",
        "Let’s assume we have a logistic regression model with two features: age (\n",
        "𝑥\n",
        "1\n",
        "x\n",
        "1\n",
        "​\n",
        " ) and income (\n",
        "𝑥\n",
        "2\n",
        "x\n",
        "2\n",
        "​\n",
        " ), and the model coefficients are as follows:\n",
        "\n",
        "𝑤\n",
        "0\n",
        "=\n",
        "−\n",
        "2\n",
        "w\n",
        "0\n",
        "​\n",
        " =−2 (intercept)\n",
        "\n",
        "𝑤\n",
        "1\n",
        "=\n",
        "0.03\n",
        "w\n",
        "1\n",
        "​\n",
        " =0.03 (coefficient for age)\n",
        "\n",
        "𝑤\n",
        "2\n",
        "=\n",
        "−\n",
        "0.0005\n",
        "w\n",
        "2\n",
        "​\n",
        " =−0.0005 (coefficient for income)\n",
        "\n",
        "The model predicts the probability of class 1 (e.g., a person buying a product). To interpret this:\n",
        "\n",
        "Intercept (\n",
        "𝑤\n",
        "0\n",
        "w\n",
        "0\n",
        "​\n",
        " ): The log-odds of class 1 when both\n",
        "𝑥\n",
        "1\n",
        "x\n",
        "1\n",
        "​\n",
        "  (age) and\n",
        "𝑥\n",
        "2\n",
        "x\n",
        "2\n",
        "​\n",
        "  (income) are 0 is -2. In terms of probability, you can convert this log-odds into probability using the sigmoid function.\n",
        "\n",
        "Age (\n",
        "𝑤\n",
        "1\n",
        "=\n",
        "0.03\n",
        "w\n",
        "1\n",
        "​\n",
        " =0.03): For every one-year increase in age, the log-odds of class 1 increase by 0.03. The odds ratio would be\n",
        "𝑒\n",
        "0.03\n",
        "≈\n",
        "1.03\n",
        "e\n",
        "0.03\n",
        " ≈1.03, meaning that for each year older a person is, the odds of them purchasing the product increase by 3%.\n",
        "\n",
        "Income (\n",
        "𝑤\n",
        "2\n",
        "=\n",
        "−\n",
        "0.0005\n",
        "w\n",
        "2\n",
        "​\n",
        " =−0.0005): For every $1 increase in income, the log-odds of class 1 decrease by 0.0005. The odds ratio would be\n",
        "𝑒\n",
        "−\n",
        "0.0005\n",
        "≈\n",
        "0.9995\n",
        "e\n",
        "−0.0005\n",
        " ≈0.9995, meaning that for every $1 increase in income, the odds of the person purchasing the product decrease by 0.05%.\n",
        "\n",
        "5. Summary of Coefficient Interpretation in Logistic Regression\n",
        "Coefficients represent the change in log-odds for a one-unit change in the corresponding feature, holding other features constant.\n",
        "\n",
        "You can exponentiate the coefficients to get the odds ratios, which are easier to interpret.\n",
        "\n",
        "Odds ratio > 1: Feature increases the odds of the positive class.\n",
        "\n",
        "Odds ratio = 1: Feature has no effect on the odds.\n",
        "\n",
        "Odds ratio < 1: Feature decreases the odds of the positive class.\n",
        "\n",
        "The intercept represents the log-odds of the outcome when all features are zero.\n",
        "\n",
        "6. Visualizing Coefficients\n",
        "Sometimes, it’s helpful to visualize the coefficients (or their corresponding odds ratios) to understand the relative importance of each feature. For example, in a model predicting customer churn, you could plot the odds ratios to show which features (e.g., customer age, tenure, or income) have the greatest impact on predicting churn.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oF45gcOChguC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "qIcQcVqB0NjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with L1 Regularization: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "3swH5-va0Y7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with L2 Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "ik4P80uX0oky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with Elastic Net regularization\n",
        "# l1_ratio=0.5 means equal mix of L1 and L2 regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "-W2schB_0zgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "\n",
        " # Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model for multiclass classification using 'ovr'\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy for Multiclass Classification (OvR):\n"
      ],
      "metadata": {
        "id": "3qE8Nl-j09yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid to tune\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Types of regularization\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support L1 and ElasticNet\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV to tune the hyperparameters\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and the best accuracy score from cross-validation\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"Best Cross-validation Accuracy: {best_score * 100:.2f}%\")\n",
        "\n",
        "# Use the best estimator from the grid search to predict on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "kA8b_WOh1G96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize StratifiedKFold with 5 splits\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy scores for each fold\n",
        "accuracy_scores = []\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    # Split the data into training and test sets for each fold\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels for the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "print(f'Average Accuracy using Stratified K-Fold Cross-Validation: {average_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "rC9KM3Lu1Qmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your actual CSV file path\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# Assuming the target variable is in the last column\n",
        "X = data.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "y = data.iloc[:, -1]   # Target (last column)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "8TV0fAOp1akQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        " # Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Regularization strength, using logarithmic scale\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Types of regularization\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support L1 and ElasticNet\n",
        "}\n",
        "\n",
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, verbose=1, random_state=42, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score from the random search\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "# Print the best parameters and the best accuracy score from cross-validation\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"Best Cross-validation Accuracy: {best_score * 100:.2f}%\")\n",
        "\n",
        "# Use the best estimator from the random search to predict on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-FSLi6Gr1kur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logreg_model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Wrap the Logistic Regression model with One-vs-One classifier\n",
        "ovo_model = OneVsOneClassifier(logreg_model)\n",
        "\n",
        "# Train the OvO model on the training data\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'One-vs-One Multiclass Logistic Regression Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "YtwDyXKf1vk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn's heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Malignant', 'Benign'], yticklabels=['Malignant', 'Benign'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Logistic Regression (Binary Classification)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xU6ZyOMx1409"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance using Precision, Recall, and F1-Score\n",
        "report = classification_report(y_test, y_pred, target_names=['Malignant', 'Benign'])\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "6jDE-F1f2FGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate an imbalanced dataset (binary classification)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                           n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with class weights set to 'balanced'\n",
        "model = LogisticRegression(max_iter=200, class_weight='balanced')\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance using Precision, Recall, F1-Score, and Confusion Matrix\n",
        "report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn's heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'],\n",
        "            yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Logistic Regression (with Class Weights)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jvM6aD_Y2S6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset (assuming the dataset is available as 'titanic.csv')\n",
        "# You can download it from Kaggle or use an in-built dataset in pandas if available\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Preview the data\n",
        "print(data.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# Fill missing numerical values with the median\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "# Fill missing categorical values with the most frequent value (mode)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "# Drop rows where 'Cabin' is missing, as it's not essential for the model\n",
        "data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Encode categorical variables\n",
        "# Convert 'Sex' into numerical values (Male = 0, Female = 1)\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "# Convert 'Embarked' into numerical values (C = 0, Q = 1, S = 2)\n",
        "data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Select features and target variable\n",
        "X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 6: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['Not Survived', 'Survived']))\n"
      ],
      "metadata": {
        "id": "GKQY_LV42cGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Logistic Regression model without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy without scaling\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f'Accuracy without scaling: {accuracy_no_scaling * 100:.2f}%')\n",
        "\n",
        "# 2. Feature scaling (Standardization) - Logistic Regression model with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy with scaling\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f'Accuracy with scaling: {accuracy_with_scaling * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "wpcJWkEn2qBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Get the probability of the positive class (class 1)\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
        "\n",
        "# Generate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (no skill classifier)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gL8F2yKg23PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17.  Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with custom C (regularization strength)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy with C=0.5: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "3ToEe24g2_0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to store feature names and their corresponding coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the features by the absolute value of their coefficients\n",
        "feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance_sorted = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Display the top 10 most important features based on the coefficients\n",
        "print(\"Top 10 most important features:\")\n",
        "print(feature_importance_sorted.head(10))\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_sorted['Feature'][:10], feature_importance_sorted['Coefficient'][:10], color='skyblue')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 10 Important Features Based on Coefficients')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y4Kps60r3JPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Display the Cohen's Kappa Score\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "fCT_z2G33TXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the predicted probabilities for the test set\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n",
        "\n",
        "# Compute precision and recall values at different thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n",
        "plt.fill_between(recall, precision, color='lightblue', alpha=0.4)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o6NOku9h3gVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Initialize a dictionary to store the accuracy for each solver\n",
        "accuracies = {}\n",
        "\n",
        "# Train Logistic Regression with each solver and evaluate accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)  # Train the model\n",
        "    y_pred = model.predict(X_test)  # Make predictions\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "    accuracies[solver] = accuracy  # Store accuracy for the current solver\n",
        "\n",
        "# Print the accuracy for each solver\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy with {solver} solver: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "r4X5KCuf3vvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance using Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the Matthews Correlation Coefficient\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "id": "S-ii4mO7351c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# 2. Standardize the data (mean = 0, std = 1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy with raw data: {accuracy_raw * 100:.2f}%\")\n",
        "print(f\"Accuracy with standardized data: {accuracy_scaled * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ZGIpDCS24EAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define a range of values for the regularization strength C\n",
        "param_grid = {'C': np.logspace(-4, 4, 20)}  # Values from 10^-4 to 10^4\n",
        "\n",
        "# Use GridSearchCV to find the optimal C using cross-validation (5-fold cross-validation)\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best value of C\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Optimal value of C: {best_C}\")\n",
        "\n",
        "# Evaluate the model with the best C on the test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with optimal C: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "nZgNK6Bc4Q-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "# Import necessary libraries\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels (binary: 0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy before saving: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Use the loaded model to make predictions on the test data\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "print(f\"Model accuracy after loading: {accuracy_loaded * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "HmV94fZK4gLb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}